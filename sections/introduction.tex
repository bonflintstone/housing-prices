\section{Introduction}
The aim of our paper is to compare two commong variations of the linear regression machine learning algorithm. Linear Regression is one of the most basic machine learning algorithms, and it works by gradually learning linear relationships between the input features and the output. Even though this is a basic algorithm, it has time and again been proven to be one of the most powerful and builds a baseline for many variations and other approaches.
Regression is a statistical machine learning approach that can be used to estimate relationships between variables; trying to predict dependent variable based on other independent variables. The most common loss function is the squared error loss funciton, but there are many different approaches and modifictions of the core linear regression idea.\newline
To evaluate these machine learning technologies, it was chosen to attempt the keggle competition "House Prices: Advanced Regression Techniques"\cite{kaggle}
In this paper two common machine learning algorithms are considered.\newline
The first one is lasso regression, which works like linear regression but adds one term to the loss function. This term is equivalent to the sum of the absolute values of the weights. Adding the weight of the features to the loss function is a common way to make the algorithm "prefer" solutions which smaller weights; in other words it is more likely to converge to a simpler solution. Lasso regression, as opposed to ridge regression, applies the absolute value of the weights instead of the square, this makes it more likely to drop the weights of uncorrelated features completely to zero, and thus removes those terms from the regression equation. This is very similar to L1 normalization.\newline
The second machine learning algorithm we used is the gradient boosting regressor. Gradient boosting works by assembling a number of weak learners step by step to create one strong learner\cite{mason_2002}.\newline

The dataset we are using is the Amex housing prices dataset, consisting of real propery sales prices with many different features for each property. Housing prices have already previously been used as one of the most common ways to test regression methods, as they tend to have a great availability of data, many different features - both categorical and numerical - and are based on real data.\newline
The approach used in this report starts by analyzing the dataset, removing outliers and processing missing data. Then the dataset is randomly split into a test and training dataset. The test dataset was not looked at anymore until the final evaluation of our learners. Then the same training set was used to train a lasso regressor and a gradient boosting regressor. After hyperparameter tuning was comleted using cross validation, which randomly re-splits the training dataset into a validation dataset and the remaining training dataset multiple times to evaluate different hyperparameters, the learners were finished. At this point we used the test dataset to evaluate the performance of our learners and compare the bias and variance.
